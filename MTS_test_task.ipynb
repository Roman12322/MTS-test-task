{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Install dependencies & model setup"
      ],
      "metadata": {
        "id": "p6T67zeR8i2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
        "# !pip install datsets transformers[sentencepiece]\n",
        "# !pip install sentencepiece\n",
        "!pip install git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q datasets bitsandbytes einops wandb"
      ],
      "metadata": {
        "id": "YPv2_79Nydfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"intent\"], examples[\"response\"],  examples[\"instruction\"],  examples[\"category\"],  truncation=True)\n",
        "\n",
        "def get_preprocessed_dataset(path_to_data, split_rate):\n",
        "  data = pd.read_csv('/content/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv')\n",
        "  idx_split = int(len(data)*split_rate)\n",
        "  data[:idx_split].to_json('json_train.json', orient='records')\n",
        "  data[idx_split:].to_json('json_test.json', orient='records')\n",
        "  data_files = {\"train\": \"json_train.json\", \"test\": \"json_test.json\"}\n",
        "  dataset_name = '/content'\n",
        "  dataset = load_dataset(dataset_name, data_files=data_files)\n",
        "  tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "  return tokenized_datasets"
      ],
      "metadata": {
        "id": "44yTvRfx8iSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "import transformers\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        "    )\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "E0Nl5mWL0k2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "a9EUEDAl0ss3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load up dataset & preprocess\n",
        "tokenized_datasets = get_preprocessed_dataset(path_to_data='/content/Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv', split_rate=0.75)"
      ],
      "metadata": {
        "id": "G_v5eFn6cmC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training process"
      ],
      "metadata": {
        "id": "stutJ4ke9IY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# api-key = 024d6bf50eff22ce75f696edb9fd651f6fb942f1\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        save_steps = 10,\n",
        "        logging_steps = 10,\n",
        "        learning_rate = 2e-4,\n",
        "        max_grad_norm = 0.3,\n",
        "        max_steps = 500,\n",
        "        warmup_ratio = 0.03,\n",
        "        lr_scheduler_type = \"constant\",\n",
        "        fp16=True,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "jq0nX33BmfaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(\"outputs\")"
      ],
      "metadata": {
        "id": "p66mZk1RAlOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test trained model  "
      ],
      "metadata": {
        "id": "RmaFjCm186pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig.from_pretrained('/content/drive/MyDrive/outputs')\n",
        "model = get_peft_model(model, lora_config).to('cuda')"
      ],
      "metadata": {
        "id": "L2Hllu-bCuN6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"### System:\\nYou are falcon-7b sharded. You are assistant for people to get information about flight, buying tickets, \\\n",
        "                luggage, refunding and other related questions.\\n\\n\"\n",
        "message = \"How can i buy a ticket to Moscow from Tokyo?\"\n",
        "prompt = f\"{system_prompt}### User: {message}\\n\\n### Assistant:\\n\"\n",
        "\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "inputs.pop('token_type_ids')\n",
        "outputs = model.generate(**inputs, do_sample=True, top_p=0.95, top_k=4, max_new_tokens=256)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "T1TiIH6vAlr_",
        "outputId": "16885100-8fd4-4e8d-93eb-bb2bfb86bdf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### System:\n",
            "You are falcon-7b sharded. You are assistant for people to get information about flight, buying tickets,                 luggage, refunding and other related questions.\n",
            "\n",
            "### User: How can i buy a ticket to Moscow from Tokyo?\n",
            "\n",
            "### Assistant:\n",
            "You need to select your departure airport - Tokyo. Then you need to select your destination airport - Moscow.\n",
            "\n",
            "You will get a list of possible flights.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}